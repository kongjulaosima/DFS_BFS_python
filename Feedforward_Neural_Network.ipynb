{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#前馈神经网络结构及前向传播\" data-toc-modified-id=\"前馈神经网络结构及前向传播-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>前馈神经网络结构及前向传播</a></span></li><li><span><a href=\"#参数学习\" data-toc-modified-id=\"参数学习-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>参数学习</a></span></li><li><span><a href=\"#误差反向传播\" data-toc-modified-id=\"误差反向传播-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>误差反向传播</a></span></li><li><span><a href=\"#梯度下降算法\" data-toc-modified-id=\"梯度下降算法-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>梯度下降算法</a></span></li><li><span><a href=\"#前馈网络的实现\" data-toc-modified-id=\"前馈网络的实现-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>前馈网络的实现</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前馈神经网络（Feedforward Neural Network，FNN）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前馈神经网络结构及前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练数据集\n",
    "\\begin{align*} \\\\& T = \\left\\{ \\left( \\mathbf{x}_{1}, y_{1} \\right), \\left( \\mathbf{x}_{2}, y_{2} \\right), \\cdots, \\left(\\mathbf{x}_i,y_i\\right),\\cdots,\\left( \\mathbf{x}_{N}, y_{N} \\right) \\right\\} \\end{align*}   \n",
    "其中，$\\mathbf{x}_{i}$为第$i$个特征向量（实例），$\\mathbf{x}_{i}=\\left( x^{\\left(1\\right)}_i,x^{\\left(2\\right)}_i,\\ldots ,x^{\\left(j\\right)}_i,\\ldots ,x^{\\left(n\\right)}_i\\right) ^{T} \\in \\mathcal{X} \\subseteq \\mathbb{R}^{n}$；$y_{i}$为$\\mathbf{x}_{i}$的类别标记，类别标记表示为类别位置为1，其余位置为0的类别向量（one-hot编码）,$\\mathbf{y}_i\\in\\{0,1\\}^m$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前馈神经网络输入层（层 1）  \\begin{align*} & \\mathbf{a}^{1}=\\left( a_{1}^{1},a_{2}^{1},\\ldots ,a_{j}^{1},\\ldots ,a_{n}^{1}\\right) ^{T}\\\\ & a_{j}^{1}=x^{\\left(j\\right)}_i\\quad\\left( j=1,2,\\ldots ,n\\right) \\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前馈神经网络隐藏层（层 2） \\begin{align*} & \\mathbf{a}^{2}=\\left( a_{1}^{2},a_{2}^{2},\\ldots ,a_{j}^{2},\\ldots  ,a_{p}^{2}\\right) ^{T}\\\\ & a_{j}^{2}=\\sigma \\left( z_{j}^{2}\\right) \\\\ & z_{j}^{2}= \\sum _{k}w_{jk}^{2}\\cdot a_{k}^{1}+b_{j}^{2}\\quad\\left( j=1,2,\\ldots ,p\\right) \\\\& \\mathbf{z}^{2}=\\left( z_{1}^{2},z_{2}^{2},\\ldots ,z_{j}^{2},\\ldots ,z_{p}^{2}\\right) ^{T}\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前馈神经网络输出层（层 3） \\begin{align*} & \\mathbf{a}^{3}=\\left( a_{1}^{3},a_{2}^{3},\\ldots ,a_{j}^{3},\\ldots,a_{m}^{3}\\right) ^{T}\\\\ & a_{j}^{3}=\\sigma \\left( z_{j}^{3}\\right) \\\\ & z_{j}^{3}= \\sum _{k}w_{jk}^{3}\\cdot a_{k}^{2}+b_{j}^{3}\\quad\\left( j=1,2,\\ldots ,m\\right) \\\\& \\mathbf{z}^{3}=\\left( z_{1}^{3},z_{2}^{3},\\ldots ,z_{j}^{3},\\ldots ,z_{m}^{3}\\right) ^{T}\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预测输出 \\begin{align*} & \\hat{\\mathbf{y}}=\\left( \\hat y_{1},\\hat y_{2},\\ldots ,\\hat y_{j},\\ldots ,\\hat y_{m}\\right) ^{T}\\\\ & \\hat y_{j}=a_{j}^{3}\\quad\\left( j=1,2,\\ldots ,m\\right)\\end{align*}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实际输出 \\begin{align*} & \\mathbf{y}=\\left( y_{1},y_{2},\\ldots ,y_{j},\\ldots ,y_{m}\\right) ^{T}  \\quad\\left( j=1,2,\\ldots ,m\\right) \\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中，$\\sigma\\left(\\cdot\\right)$为激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单个实例$\\mathbf{x}$的损失函数$L_{\\mathbf{x}}\\left(\\mathbf{y},\\hat{\\mathbf{y}}\\right)$为平方损失函数、\n",
    "$$\\begin{align*} & C_{\\mathbf{x}}=\\dfrac {1} {2}\\left\\| \\mathbf{y}-\\hat {\\mathbf{y}}\\right\\| ^{2}=\\dfrac {1} {2}\\sum _{j}\\left( y_{j}-\\hat {y}_{j}\\right) ^{2} \\end{align*} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "目标函数（经验风险）  \\begin{align*} & C=\\dfrac {1} {N}\\sum _{\\mathbf{x}}C_{\\mathbf{x}} \\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第$l$层参数$w_{jk}^l$更新为  \n",
    "$$\\begin{align}\n",
    "w_{jk}^l & \\leftarrow w_{jk}^l-\\alpha \\dfrac {\\partial C} {\\partial w_{jk}^{l}}  \\\\\n",
    "&=w_{jk}^l-\\alpha\\frac{1}{N}\\sum_{\\mathbf{x}}\\dfrac {\\partial C_x} {\\partial w_{jk}^{l}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第$l$层参数$b_j^l$更新为    \n",
    "$$\\begin{align}\n",
    "b_j^l & \\leftarrow b_j^l-\\alpha \\dfrac {\\partial C} {\\partial b_j^{l}}  \\\\\n",
    "&=b_j^l-\\alpha\\frac{1}{N}\\sum_{\\mathbf{x}}\\dfrac {\\partial C_x} {\\partial b_j^{l}}\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义第$l$层的第$j$个神经元上的误差 \\begin{align*} & \\delta _{j}^{l}\\equiv \\dfrac {\\partial C_{\\mathbf{x}}} {\\partial z_{j}^{l}} \\quad\\left( l=2,3\\right)\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输出层误差 \n",
    "$$\\begin{align} \\delta _{j}^{3}&=\\dfrac {\\partial C_{\\mathbf{x}}} {\\partial z_{j}^{3}} \\\\ \n",
    "& =\\dfrac {\\partial C_{\\mathbf{x}}} {\\partial a_{j}^{3}}\\cdot\\dfrac {\\partial a_{j}^{3}} {\\partial z_{j}^{3}} \\\\ & =\\dfrac {\\partial \\left(\\sum_{j=1}^m \\left(y_j-\\hat{y}_j\\right)^2\\right)} {\\partial \\hat{y}_j}\\cdot \\sigma '\\left( z_{j}^{3}\\right) \\\\\n",
    "& = \\left(\\hat{y}_j-y_j\\right) \\cdot \\sigma'\\left( z_{j}^{3} \\right)\\quad\\left( j=1,2,\\ldots ,m\\right)\\end{align}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "隐藏层误差\n",
    "$$\\begin{align} \\delta _{j}^{2}&=\\dfrac {\\partial C_{\\mathbf{x}}} {\\partial z_{j}^{2}}\\\\ \n",
    "& =\\sum _{k}\\dfrac {\\partial C_{\\mathbf{x}}} {\\partial z_{k}^{3}}\\cdot \\dfrac {\\partial z_{k}^{3}} {\\partial z_{j}^{2}} \\\\ \n",
    "& = \\sum _{k} \\dfrac {\\partial z_{k}^{3}} {\\partial z_{j}^{2}}\\cdot\\delta _{k}^{3}\\\\ \n",
    "& = \\sum _{k} \\dfrac {\\partial \\left( \\sum _{j}w_{kj}^{3}\\cdot a_{j}^{2}+b_{k}^{3}\\right)} {\\partial z_{j}^{2}}\\cdot\\delta _{k}^{3}\\\\ \n",
    "& = \\sum _{k} \\dfrac {\\partial \\left( \\sum _{j}w_{kj}^{3}\\cdot  \\sigma \\left( z_{j}^{2}\\right)+b_{k}^{3}\\right)} {\\partial z_{j}^{2}}\\cdot\\delta _{k}^{3}\\\\ \n",
    "& = \\sum _{k} w_{kj}^{3}\\cdot \\sigma '\\left( z_{j}^{2}\\right) \\cdot\\delta _{k}^{3} \\\\ \n",
    "& = \\sigma '\\left( z_{j}^{2}\\right) \\cdot\\sum _{k} w_{kj}^{3} \\delta _{k}^{3} \\quad\\left( j=1,2,\\ldots ,p\\right)\\quad\\left( k=1,2,\\ldots ,m\\right)\n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "损失函数在隐藏层（层2）／输出层（层3）关于偏置的梯度 \\begin{align*} & \\dfrac {\\partial C_{\\mathbf{x}}} {\\partial b_{j}^{l}}=\\dfrac {\\partial C_{\\mathbf{x}}} {\\partial z_{j}^{l}}\\cdot \\dfrac {\\partial z_{j}^{l}} {\\partial b_{j}^{l}}=\\delta _{j}^{l}\\cdot \\dfrac {\\partial \\left( \\sum _{k}w_{jk}^{l}a_{k}^{l-1}+b_{j}^{l}\\right) } {\\partial b_{j}^{l}}=\\delta _{j}^{l}\\quad\\left( l=2,3\\right)\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数在隐藏层（层2）／输出层（层3）关于权值的梯度\\begin{align*} & \\dfrac {\\partial C_{\\mathbf{x}}} {\\partial w_{jk}^{l}}=\\dfrac {\\partial C_{\\mathbf{x}}} {\\partial z_{j}^{l}}\\cdot \\dfrac {\\partial z_{j}^{l}} {\\partial w_{jk}^{l}}=\\delta _{j}^{l}\\cdot \\dfrac {\\partial \\left( \\sum _{k}w_{jk}^{l}a_{k}^{l-1}+b_{j}^{l}\\right) } {\\partial w_{jk}^{l}}=\\delta _{j}^{l}\\cdot a_{k}^{l-1}\\quad\\left( l=2,3\\right)\\end{align*} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法4.1 偏置与权值的梯度计算算法  \n",
    "输入：实例$\\mathbf{x}=\\left(x^{\\left(1\\right)},\\cdots,x^{\\left(n\\right)}\\right)^\\top$    \n",
    "输出：损失函数在隐藏层（层2）／输出层（层3）关于偏置及权值的梯度$\\left(\\dfrac {\\partial C_{\\mathbf{x}}} {\\partial b_{j}^{l}}\\right)$和$\\left(\\dfrac {\\partial C_{\\mathbf{x}}} {\\partial w_{jk}^{l}}\\right)$   \n",
    "1. 为输入层设置对应的激活值$\\left(a_j^1\\right)_{j=1}^n=\\left(x^{\\left(j\\right)}\\right)_{j=1}^n$ \n",
    "2.  前向传播：对每个$l（l=2,3）$计算\\begin{align*} &a_{j}^{l}=\\sigma \\left( z_{j}^{l}\\right) \\\\ & z_{j}^{l}= \\sum _{k}w_{jk}^{l}\\cdot a_{k}^{l-1}+b_{j}^{l}\\end{align*}  \n",
    "3. 计算输出层误差$\\left(\\delta _{j}^{3}\\right)_{j=1}^m$；  \n",
    "4. 计算误差反向传播：隐藏层误差$\\left(\\delta _{j}^{2}\\right)_{j=1}^p$；  \n",
    "5. 计算损失函数在隐藏层（层2）／输出层（层3）关于偏置及权值的梯度$\\left(\\dfrac {\\partial C_{\\mathbf{x}}} {\\partial b_{j}^{l}}\\right)$和$\\left(\\dfrac {\\partial C_{\\mathbf{x}}} {\\partial w_{jk}^{l}}\\right)$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法4.2 梯度下降算法：  \n",
    "输入：训练实例集合$T$  \n",
    "输出：偏置和权值的更新\n",
    "1. 对每个训练实例$\\mathbf{x}$：设置对应的输入激活$\\mathbf{a}^{\\mathbf{x},1}$，并执行以下步骤：\n",
    "    + 前向传播：计算$\\mathbf{z}^{\\mathbf{x},l}=\\mathbf{W}^l\\mathbf{a}^{\\mathbf{x},l-1}+\\mathbf{b}^l$及$\\mathbf{a}^{\\mathbf{x},l}=\\sigma\\left(\\mathbf{z}^{\\mathbf{x},l}\\right)$，其中$l=2,3,\\cdots,L$。\n",
    "    + 输出层误差：$\\delta^{\\mathbf{x},L}=\\nabla_{\\mathbf{a}^L}C_\\mathbf{x}\\odot\\sigma'\\left(\\mathbf{z}^{\\mathbf{x},L}\\right)$\n",
    "    + 误差反向传播：对每个$l=L-1,L-2,\\cdots,2$，计算$\\delta^{\\mathbf{x},l}=\\left(\\left(\\mathbf{W}^{l+1}\\right)^\\top\\delta^{\\mathbf{x},l+1}\\right)\\odot\\sigma'\\left(\\mathbf{z}^{\\mathbf{x},l}\\right)$。\n",
    "2. 梯度下降：对每个$l=L-1,L-2,\\cdots,2$，根据$\\mathbf{W}^l\\leftarrow\\mathbf{W}^l-\\frac{\\eta}{m}\\sum_\\mathbf{x}\\delta^{\\mathbf{x},l}\\left(\\mathbf{a}^{\\mathbf{x},l-1}\\right)^\\top$和$\\mathbf{b}^l\\leftarrow\\mathbf{b}^l-\\frac{\\eta}{m}\\sum_\\mathbf{x}\\delta^{\\mathbf{x},l}$更新权重和偏置。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前馈网络的实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入相关Python包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T09:02:04.082762Z",
     "start_time": "2019-09-22T09:02:01.113377Z"
    }
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import svm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T10:59:09.377447Z",
     "start_time": "2019-09-18T10:59:09.368199Z"
    }
   },
   "source": [
    "加载MNIST数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-22T09:02:08.966263Z",
     "start_time": "2019-09-22T09:02:07.934013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,)\n",
      "(10000, 784) (10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    f = gzip.open('./data/mnist.pkl.gz')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "    f.close()\n",
    "    return training_data, validation_data, test_data\n",
    "\n",
    "training_data, validation_data, test_data = load_data()\n",
    "print(training_data[0].shape, training_data[1].shape)\n",
    "print(test_data[0].shape, test_data[1].shape)\n",
    "\n",
    "image = training_data[0][0].reshape(28, 28)\n",
    "plt.imshow(image)\n",
    "plt.show()\n",
    "\n",
    "print(training_data[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建支持向量机SVM模型，作为人工神经网络模型的性能对比的基模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T07:39:52.370662Z",
     "start_time": "2019-09-18T07:39:52.355996Z"
    }
   },
   "outputs": [],
   "source": [
    "def svm_baseline():\n",
    "    training_data, validation_data, test_data = load_data()\n",
    "    \n",
    "    clf = svm.SVC()\n",
    "    clf.fit(training_data[0], training_data[1])\n",
    "    \n",
    "    predictions = [int(a) for a in clf.predict(test_data[0])]\n",
    "    num_correct = sum(int(a == y) for a, y in zip(predictions, test_data[1]))\n",
    "    print(\"Baseline classifier using an SVM.\")\n",
    "    print(str(num_correct) + \" of \" + str(len(test_data[1])) + \" values correct.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T07:56:00.259176Z",
     "start_time": "2019-09-18T07:39:55.047473Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/dev/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline classifier using an SVM.\n",
      "9435 of 10000 values correct.\n"
     ]
    }
   ],
   "source": [
    "svm_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "封装训练数据集、验证数据集和测试数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T11:02:36.606858Z",
     "start_time": "2019-09-18T11:02:36.589902Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_wrapper():\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    \n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    \n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(training_inputs, va_d[1])\n",
    "    \n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    \n",
    "    return training_data, validation_data, test_data\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T11:03:47.688537Z",
     "start_time": "2019-09-18T11:03:46.672273Z"
    }
   },
   "outputs": [],
   "source": [
    " training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义前馈神经网络模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T11:04:28.163774Z",
     "start_time": "2019-09-18T11:04:28.138344Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a) + b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "            \n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [training_data[k: k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "                \n",
    "            if test_data:\n",
    "                print(\"Epoch {} : {} / {}\".format(j, self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} complete\".format(j))\n",
    "                \n",
    "    def update_mini_batch(self, mini_batch, eta):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb + dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw + dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "            \n",
    "        self.biases = [b - (eta / len(mini_batch)) * nb for b, nb in zip(self.biases, nabla_b)]\n",
    "        self.weights = [w - (eta / len(mini_batch)) * nw for w, nw in zip(self.weights, nabla_w)]\n",
    "    \n",
    "    def backprop(self, x, y):\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        activation = x\n",
    "        activations = [x]\n",
    "        zs = []\n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation) + b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "            \n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "            \n",
    "        return nabla_b, nabla_w\n",
    "    \n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        return output_activations - y\n",
    "        \n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "    \n",
    "def sigmoid(z):\n",
    "    return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建输入层784个结点，隐藏层30个结点，输出层10个结点的三层前馈神经网络，使用训练数据集进行模型训练，使用测试数据集进行测试："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-18T11:09:01.267303Z",
     "start_time": "2019-09-18T11:06:33.336133Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : 9093 / 10000\n",
      "Epoch 1 : 9254 / 10000\n",
      "Epoch 2 : 9324 / 10000\n",
      "Epoch 3 : 9354 / 10000\n",
      "Epoch 4 : 9421 / 10000\n",
      "Epoch 5 : 9383 / 10000\n",
      "Epoch 6 : 9431 / 10000\n",
      "Epoch 7 : 9422 / 10000\n",
      "Epoch 8 : 9436 / 10000\n",
      "Epoch 9 : 9460 / 10000\n",
      "Epoch 10 : 9457 / 10000\n",
      "Epoch 11 : 9462 / 10000\n",
      "Epoch 12 : 9479 / 10000\n",
      "Epoch 13 : 9467 / 10000\n",
      "Epoch 14 : 9432 / 10000\n",
      "Epoch 15 : 9453 / 10000\n",
      "Epoch 16 : 9469 / 10000\n",
      "Epoch 17 : 9479 / 10000\n",
      "Epoch 18 : 9493 / 10000\n",
      "Epoch 19 : 9484 / 10000\n",
      "Epoch 20 : 9510 / 10000\n",
      "Epoch 21 : 9498 / 10000\n",
      "Epoch 22 : 9492 / 10000\n",
      "Epoch 23 : 9480 / 10000\n",
      "Epoch 24 : 9482 / 10000\n",
      "Epoch 25 : 9463 / 10000\n",
      "Epoch 26 : 9485 / 10000\n",
      "Epoch 27 : 9499 / 10000\n",
      "Epoch 28 : 9519 / 10000\n",
      "Epoch 29 : 9506 / 10000\n"
     ]
    }
   ],
   "source": [
    "net = Network([784,30,10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma\\left(x\\right)=\\frac{1}{1+\\exp\\left(-x\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic函数将实域输入映射到区间$\\left(0,1\\right)$，且连续可导，其输出可以看做是概率分布，可作为柔性门以控制其他输出信息量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-Logistic激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic函数的导数为\n",
    "$$\\sigma'\\left(x\\right)=\\sigma\\left(x\\right)\\left(1-\\sigma\\left(x\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic函数在$0$附近的一阶泰勒展开为\n",
    "$$\\begin{align}\n",
    "g_l\\left(x\\right)&\\thickapprox\\sigma\\left(0\\right)+x\\sigma\\left(x\\right)  \\\\\n",
    "&=0.25x+0.5\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以分段函数近似Logistic函数，得到\n",
    "$$\n",
    "\\begin{aligned} hard\\_logistic(x) &=\\left\\{\n",
    "\\begin{array}\n",
    "{ll}{1} & {g_{l}(x) \\geq 1} \\\\ \n",
    "{g_{l}} & {0<g_{l}(x)<1} \\\\ \n",
    "{0} & {g_{l}(x) \\leq 0}\n",
    "\\end{array}\n",
    "\\right.\\\\ \n",
    "&=\\max \\left(\\min \\left(g_{l}(x), 1\\right), 0\\right) \\\\\n",
    "&=\\max \\left(\\min \\left(0.25x+0.5x, 1\\right), 0\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tanh\\left(x\\right)=\\frac{\\exp\\left(x\\right)-\\exp\\left(-x\\right)}{\\exp\\left(x\\right)+\\exp\\left(-x\\right)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh函数是放大并平移的Logistic函数，\n",
    "$$\\tanh\\left(x\\right)=2\\sigma\\left(2x\\right)-1$$\n",
    "其值域是$\\left(-1,1\\right)$，输出是零中心化的（Zero-Centered）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard-Tanh激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh函数的导数为\n",
    "$$\\tanh'\\left(x\\right)=1-\\left(\\tanh\\left(x\\right)\\right)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh函数在$0$附近的一阶泰勒展开为\n",
    "$$\\begin{align}\n",
    "g_t&\\thickapprox \\tanh\\left(0\\right)+\\tanh'\\left(x\\right)x   \\\\\n",
    "&=x\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以分段函数近似Tanh函数，得到\n",
    "$$\\begin{align}\n",
    "hard\\_tanh\\left(x\\right)&=\\max\\left(\\min\\left(g_t\\left(x\\right),1\\right),-1\\right)  \\\\\n",
    "&=\\max\\left(\\min\\left(x,1\\right),-1\\right)\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修正线性单元（Rectified Linear Unit，ReLU）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "修正线性单元ReLU（rectifier激活函数）是一个斜坡函数，\n",
    "$$\\begin{aligned} ReLU(x) &=\\left\\{\n",
    "\\begin{array}\n",
    "{ll}{x} & x \\geq 0 \\\\ \n",
    "{0} & {x<0} \n",
    "\\end{array}\n",
    "\\right.   \\\\\n",
    "&=\\max\\left(0,x\\right)\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带泄露的修正线性单元（Leaky ReLU）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带泄露的修正线性单元在输入$x<0$时，保持一个很小的梯度$\\gamma$\n",
    "$$\\begin{aligned} Leaky\\_ReLU(x) &=\\left\\{\n",
    "\\begin{array}\n",
    "{ll}{x} & x > 0 \\\\ \n",
    "{\\gamma x} & {x\\leq 0} \n",
    "\\end{array}\n",
    "\\right.   \\\\\n",
    "&=\\max\\left(0,x\\right)+\\gamma\\min\\left(0,x\\right)\n",
    "\\end{aligned}$$\n",
    "其中，$\\gamma$是一个较小的常数，例如$0.01$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当$\\gamma<1$时，可记为\n",
    "$$Leaky\\_ReLU\\left(x\\right)=max\\left(x,\\gamma x\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 带参数的修正线性单元（Parametric ReLU, PReLU）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "单参数的修正线性单元为每个神经元引入一个可学习的参数（一组神经元也可以共享一个参数）\n",
    "$$\\begin{aligned}PReLU(x) &=\\left\\{\n",
    "\\begin{array}\n",
    "{ll}{x} & x > 0 \\\\ \n",
    "{\\gamma_i x} & {x\\leq 0} \n",
    "\\end{array}\n",
    "\\right.   \\\\\n",
    "&=\\max\\left(0,x\\right)+\\gamma_i \\min\\left(0,x\\right)\n",
    "\\end{aligned}$$\n",
    "其中，$\\gamma_i$为$x\\leq0$时函数的斜率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 指数线性单元（Exponential Linear Unit, ELU）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "指数线性单元是一个近似的零中心化的非线性函数，\n",
    "$$\\begin{aligned} ELU(x) &=\\left\\{\n",
    "\\begin{array}\n",
    "{ll}{x} & x > 0 \\\\ \n",
    "{\\gamma\\left(\\exp\\left(x\\right)-1\\right)} & {x\\leq 0} \n",
    "\\end{array}\n",
    "\\right.   \\\\\n",
    "&=\\max\\left(0,x\\right)+\\min\\left(0,\\gamma\\left(\\exp\\left(x\\right)-1\\right)\\right)\n",
    "\\end{aligned}$$\n",
    "其中，$\\gamma\\geq0$是一个超参数，决定$x\\leq0$时的饱和曲线，并调整输出均值在$0$附近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softplus激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Softplus激活函数可以看做是ReLU的平滑版本\n",
    "$$softplus\\left(x\\right)=\\log\\left(1+\\exp\\left(x\\right)\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swish激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Swish激活函数是一种门控激活函数\n",
    "$$swish\\left(x\\right)=x\\sigma\\left(\\beta x\\right)$$\n",
    "其中，$\\sigma\\left(\\cdot\\right)$为Logistic函数，$\\beta$为可学习的参数或固定的超参数。$\\sigma\\left(\\cdot\\right)\\in\\left(0,1\\right)$可以看作是软性门控机制。当$\\sigma\\left(\\beta x\\right)$接近于$1$时，激活函数输出近似于$x$；当$\\sigma\\left(\\beta x\\right)$接近于$0$时，激活函数输出近似于$0$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxout单元"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maxout单元是分段线性单元，输入是上一层神经元的全部输入$\\mathbf{x}=\\left[x_1,\\cdots,x_n\\right]^\\top$。Maxout单元有$K$个权重向量$\\mathbf{w}_k\\in\\mathbb{R}^n$和偏置$b_k\\in\\mathbb{R}$。对于输入$\\mathbf{x}$，可以得到$K$个净输入$z_k$\n",
    "$$z_k=\\mathbf{w}_k^\\top\\mathbf{x}+b_k$$\n",
    "其中，$\\mathbf{w}_k=\\left[w_{k,1},\\cdots,w_{k,n}\\right]^\\top$为第$k$个权重向量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maxout单元\n",
    "$$maxout\\left(\\mathbf{x}\\right)=\\max_{k\\in\\left[1,K\\right]}\\left(z_k\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maxout单元整体学习输入到输出之间的非线性映射关系。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
